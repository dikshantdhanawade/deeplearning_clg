{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRTRkpF1UJ28YyRVYLh6+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dikshantdhanawade/deeplearning_clg/blob/main/DL_Exp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wExznBY6JtVq",
        "outputId": "1658af65-6ac5-4e28-f59c-a5cba5021ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Batch GD] Epoch 0, Loss: 34.00539302415556\n",
            "[Batch GD] Epoch 10, Loss: 20.179177147674203\n",
            "[Batch GD] Epoch 20, Loss: 15.13373966847006\n",
            "[Batch GD] Epoch 30, Loss: 11.765365386361438\n",
            "[Batch GD] Epoch 40, Loss: 9.516611863770882\n",
            "[Batch GD] Epoch 50, Loss: 8.015326085577074\n",
            "[Batch GD] Epoch 60, Loss: 7.01305573217234\n",
            "[Batch GD] Epoch 70, Loss: 6.343932054385539\n",
            "[Batch GD] Epoch 80, Loss: 5.897219753004988\n",
            "[Batch GD] Epoch 90, Loss: 5.598991059522268\n",
            "[Batch GD] Epoch 100, Loss: 5.399891206347097\n",
            "[Batch GD] Epoch 110, Loss: 5.266970557192083\n",
            "[Batch GD] Epoch 120, Loss: 5.178231672205335\n",
            "[Batch GD] Epoch 130, Loss: 5.118988885183523\n",
            "[Batch GD] Epoch 140, Loss: 5.079437928298772\n",
            "[Batch GD] Epoch 150, Loss: 5.053033394191969\n",
            "[Batch GD] Epoch 160, Loss: 5.035405516731789\n",
            "[Batch GD] Epoch 170, Loss: 5.023637005214251\n",
            "[Batch GD] Epoch 180, Loss: 5.015780253109451\n",
            "[Batch GD] Epoch 190, Loss: 5.010535022772183\n",
            "Output of DeepLearning Model: [[4.91613544]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "layers = 10\n",
        "\n",
        "weights = []\n",
        "biases = []\n",
        "\n",
        "for i in range(layers):\n",
        "  w = np.random.randn(1,1)\n",
        "  b = np.random.uniform(0.1, 0.5)\n",
        "  weights.append(w)\n",
        "  biases.append(b)\n",
        "\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "     return np.maximum(0, x)\n",
        "\n",
        "# def tanh(x):\n",
        "#   return np.tanh(x)\n",
        "\n",
        "# def sigmoid(x):\n",
        "#   return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def mse_grad(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "    activations = [x]\n",
        "    pre_acts = []\n",
        "\n",
        "    for i in range(layers):\n",
        "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
        "        pre_acts.append(z)\n",
        "\n",
        "        if i == layers - 1:\n",
        "            x = z\n",
        "        else:\n",
        "            x = relu(z)\n",
        "\n",
        "        activations.append(x)\n",
        "\n",
        "    return activations, pre_acts\n",
        "\n",
        "\n",
        "def backward(activations, pre_acts, y_true):\n",
        "    grads_w = []\n",
        "    grads_b = []\n",
        "\n",
        "    grad = mse_grad(activations[-1], y_true)\n",
        "\n",
        "    for i in reversed(range(layers)):\n",
        "        if i == layers - 1:\n",
        "            dz = grad              # no activation derivative\n",
        "        else:\n",
        "            dz = grad * (pre_acts[i] > 0)\n",
        "\n",
        "        dw = np.dot(activations[i].T, dz)\n",
        "        db = dz.mean(axis=0, keepdims=True)\n",
        "\n",
        "        grads_w.insert(0, dw)\n",
        "        grads_b.insert(0, db)\n",
        "\n",
        "        grad = np.dot(dz, weights[i].T)\n",
        "\n",
        "    return grads_w, grads_b\n",
        "\n",
        "\n",
        "def batch_gd(X, Y, lr=0.01, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        acts, pre = forward(X)\n",
        "        grads_w, grads_b = backward(acts, pre, Y)\n",
        "\n",
        "        for i in range(layers):\n",
        "            weights[i] -= lr * grads_w[i]\n",
        "            biases[i] -= lr * grads_b[i]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss = mse_loss(acts[-1], Y)\n",
        "            print(f\"[Batch GD] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "X = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "Y = np.array([[2.0], [4.0], [6.0], [8.0]])\n",
        "\n",
        "batch_gd(X, Y, lr=0.01, epochs=200)\n",
        "\n",
        "x = np.array([[1.0]])\n",
        "acts, _ = forward(x)\n",
        "print(\"Output of DeepLearning Model:\", acts[-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "layers = 10\n",
        "\n",
        "weights = []\n",
        "biases = []\n",
        "\n",
        "for i in range(layers):\n",
        "  w = np.random.randn(1,1)\n",
        "  b = np.random.uniform(0.1, 0.5)\n",
        "  weights.append(w)\n",
        "  biases.append(b)\n",
        "\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "     return np.maximum(0, x)\n",
        "\n",
        "# def tanh(x):\n",
        "#   return np.tanh(x)\n",
        "\n",
        "# def sigmoid(x):\n",
        "#   return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def mse_grad(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "    activations = [x]\n",
        "    pre_acts = []\n",
        "\n",
        "    for i in range(layers):\n",
        "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
        "        pre_acts.append(z)\n",
        "\n",
        "        if i == layers - 1:\n",
        "            x = z\n",
        "        else:\n",
        "            x = relu(z)\n",
        "\n",
        "        activations.append(x)\n",
        "\n",
        "    return activations, pre_acts\n",
        "\n",
        "\n",
        "def backward(activations, pre_acts, y_true):\n",
        "    grads_w = []\n",
        "    grads_b = []\n",
        "\n",
        "    grad = mse_grad(activations[-1], y_true)\n",
        "\n",
        "    for i in reversed(range(layers)):\n",
        "        if i == layers - 1:\n",
        "            dz = grad\n",
        "        else:\n",
        "            dz = grad * (pre_acts[i] > 0)\n",
        "\n",
        "        dw = np.dot(activations[i].T, dz)\n",
        "        db = dz.mean(axis=0, keepdims=True)\n",
        "\n",
        "        grads_w.insert(0, dw)\n",
        "        grads_b.insert(0, db)\n",
        "\n",
        "        grad = np.dot(dz, weights[i].T)\n",
        "\n",
        "    return grads_w, grads_b\n",
        "\n",
        "\n",
        "\n",
        "def batch_gd(X, Y, lr=0.01, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        acts, pre = forward(X)\n",
        "        grads_w, grads_b = backward(acts, pre, Y)\n",
        "\n",
        "        for i in range(layers):\n",
        "            weights[i] -= lr * grads_w[i]\n",
        "            biases[i] -= lr * grads_b[i]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss = mse_loss(acts[-1], Y)\n",
        "            print(f\"[Batch GD] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "def stochastic_gd(X, Y, lr=0.01, epochs=100):\n",
        "    n = X.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(n)\n",
        "        X_shuffled = X[indices]\n",
        "        Y_shuffled = Y[indices]\n",
        "\n",
        "        for i in range(n):\n",
        "            x_i = X_shuffled[i:i+1]\n",
        "            y_i = Y_shuffled[i:i+1]\n",
        "\n",
        "            acts, pre = forward(x_i)\n",
        "            grads_w, grads_b = backward(acts, pre, y_i)\n",
        "\n",
        "            for j in range(layers):\n",
        "                weights[j] -= lr * grads_w[j]\n",
        "                biases[j] -= lr * grads_b[j]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            acts, _ = forward(X)\n",
        "            loss = mse_loss(acts[-1], Y)\n",
        "            print(f\"[SGD] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "def mini_batch_gd(X, Y, batch_size=2, lr=0.01, epochs=100):\n",
        "    n = X.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(n)\n",
        "        X_shuffled = X[indices]\n",
        "        Y_shuffled = Y[indices]\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            x_batch = X_shuffled[i:i+batch_size]\n",
        "            y_batch = Y_shuffled[i:i+batch_size]\n",
        "\n",
        "            acts, pre = forward(x_batch)\n",
        "            grads_w, grads_b = backward(acts, pre, y_batch)\n",
        "\n",
        "            for j in range(layers):\n",
        "                weights[j] -= lr * grads_w[j]\n",
        "                biases[j] -= lr * grads_b[j]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            acts, _ = forward(X)\n",
        "            loss = mse_loss(acts[-1], Y)\n",
        "            print(f\"[Mini-Batch] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "X = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
        "Y = np.array([[2.0], [4.0], [6.0], [8.0]])\n",
        "\n",
        "# batch_gd(X, Y, lr=0.01, epochs=200)\n",
        "# stochastic_gd(X, Y, lr=0.01, epochs=200)\n",
        "mini_batch_gd(X, Y, batch_size=2, lr=0.01, epochs=200)\n",
        "\n",
        "\n",
        "\n",
        "x = np.array([[1.0]])\n",
        "acts, _ = forward(x)\n",
        "print(\"Output of DeepLearning Model:\", acts[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpGjmUcuSy3q",
        "outputId": "d41d7989-6d5c-4f37-d787-ba6ca8c80dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Mini-Batch] Epoch 0, Loss: 25.93129991767843\n",
            "[Mini-Batch] Epoch 10, Loss: 14.330774016773091\n",
            "[Mini-Batch] Epoch 20, Loss: 9.149630501369408\n",
            "[Mini-Batch] Epoch 30, Loss: 6.852487617871317\n",
            "[Mini-Batch] Epoch 40, Loss: 5.823931787020676\n",
            "[Mini-Batch] Epoch 50, Loss: 5.366948680410291\n",
            "[Mini-Batch] Epoch 60, Loss: 5.1626853524429865\n",
            "[Mini-Batch] Epoch 70, Loss: 5.073089149019298\n",
            "[Mini-Batch] Epoch 80, Loss: 5.032748393238853\n",
            "[Mini-Batch] Epoch 90, Loss: 5.014091013900508\n",
            "[Mini-Batch] Epoch 100, Loss: 5.005947019674549\n",
            "[Mini-Batch] Epoch 110, Loss: 5.002495823704361\n",
            "[Mini-Batch] Epoch 120, Loss: 5.001039623974114\n",
            "[Mini-Batch] Epoch 130, Loss: 5.000628655750512\n",
            "[Mini-Batch] Epoch 140, Loss: 5.000310085121734\n",
            "[Mini-Batch] Epoch 150, Loss: 5.000153359452714\n",
            "[Mini-Batch] Epoch 160, Loss: 5.000066368129019\n",
            "[Mini-Batch] Epoch 170, Loss: 5.000016636685922\n",
            "[Mini-Batch] Epoch 180, Loss: 5.000004246066053\n",
            "[Mini-Batch] Epoch 190, Loss: 5.000007700053356\n",
            "Output of DeepLearning Model: [[4.99749671]]\n"
          ]
        }
      ]
    }
  ]
}